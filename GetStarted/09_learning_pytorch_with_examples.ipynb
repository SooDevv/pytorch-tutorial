{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning pytorch with examples \n",
    "- this notebook introduces the fundamental concepts of PyTorch through self-contained examples\n",
    "<p>\n",
    "    \n",
    "including ...\n",
    "- Tensors\n",
    "- Autograd\n",
    "- Defining new autograd functions\n",
    "- nn module\n",
    "- optim\n",
    "- custom nn module\n",
    "- Control Flow + Weight Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 30550786.678252764\n",
      "1 25110953.947662655\n",
      "2 21661292.91406838\n",
      "3 17791799.747742258\n",
      "4 13489025.448792547\n",
      "5 9392729.094840549\n",
      "6 6220181.915184816\n",
      "7 4052364.2697424153\n",
      "8 2698973.3105222657\n",
      "9 1872115.8524401088\n",
      "10 1364174.8259402183\n",
      "11 1040819.2371894151\n",
      "12 824925.8035429991\n",
      "13 672437.2805064964\n",
      "14 559520.7158192876\n",
      "15 472486.4588767201\n",
      "16 403249.1516163831\n",
      "17 346887.9587069084\n",
      "18 300263.7692678326\n",
      "19 261202.6386545786\n",
      "20 228194.39681613835\n",
      "21 200095.44355836613\n",
      "22 176059.88017745316\n",
      "23 155369.21807537504\n",
      "24 137490.5364258464\n",
      "25 121982.82436266367\n",
      "26 108486.96103980645\n",
      "27 96713.69829501865\n",
      "28 86401.66662157446\n",
      "29 77356.74874203437\n",
      "30 69393.73174146927\n",
      "31 62358.41654543238\n",
      "32 56141.94571395784\n",
      "33 50630.32319654829\n",
      "34 45732.521186704806\n",
      "35 41374.81463894256\n",
      "36 37489.2288511774\n",
      "37 34014.95471840665\n",
      "38 30903.133608181823\n",
      "39 28113.521784802448\n",
      "40 25611.089416310882\n",
      "41 23359.500844502116\n",
      "42 21329.90481229312\n",
      "43 19496.829765893017\n",
      "44 17840.247736029087\n",
      "45 16341.184948237747\n",
      "46 14982.297757163127\n",
      "47 13748.634347251547\n",
      "48 12627.83827146295\n",
      "49 11608.889336856551\n",
      "50 10682.003311849594\n",
      "51 9837.207436003897\n",
      "52 9066.560580611556\n",
      "53 8363.06345626703\n",
      "54 7720.1687072547675\n",
      "55 7132.014980913598\n",
      "56 6593.314124765679\n",
      "57 6099.732757478081\n",
      "58 5647.130240433012\n",
      "59 5232.15646976886\n",
      "60 4852.106194738106\n",
      "61 4502.562072069739\n",
      "62 4180.692661414694\n",
      "63 3884.122031500645\n",
      "64 3610.79596275937\n",
      "65 3358.6989414265395\n",
      "66 3125.8402948064236\n",
      "67 2910.7359677053173\n",
      "68 2711.9191171696802\n",
      "69 2527.894352786803\n",
      "70 2357.5326149465\n",
      "71 2199.735508868809\n",
      "72 2053.532501427107\n",
      "73 1917.9278131664184\n",
      "74 1792.0977090817028\n",
      "75 1675.2859783159427\n",
      "76 1566.7664255788625\n",
      "77 1465.8687896271326\n",
      "78 1372.0589469942988\n",
      "79 1284.7704720815454\n",
      "80 1203.518187850208\n",
      "81 1127.8251147930926\n",
      "82 1057.298045297317\n",
      "83 991.5576798027719\n",
      "84 930.247432088833\n",
      "85 873.0384895254161\n",
      "86 819.6207867914547\n",
      "87 769.7425973737134\n",
      "88 723.1432200827329\n",
      "89 679.5842486776681\n",
      "90 638.834830021086\n",
      "91 600.7144645811356\n",
      "92 565.048273607287\n",
      "93 531.6472905633875\n",
      "94 500.36834454366567\n",
      "95 471.0748282065207\n",
      "96 443.61963247757785\n",
      "97 417.86535338447976\n",
      "98 393.7124186701664\n",
      "99 371.04983823041243\n",
      "100 349.7831858067156\n",
      "101 329.81310935283403\n",
      "102 311.06152126708673\n",
      "103 293.44203890555735\n",
      "104 276.885467941703\n",
      "105 261.3262515979812\n",
      "106 246.69371324308594\n",
      "107 232.92618948096558\n",
      "108 219.97501757787273\n",
      "109 207.79148920690258\n",
      "110 196.318345994757\n",
      "111 185.5169457833174\n",
      "112 175.34231072363937\n",
      "113 165.7608305335742\n",
      "114 156.732660410876\n",
      "115 148.21749697366204\n",
      "116 140.19254242132507\n",
      "117 132.62608067642094\n",
      "118 125.49151796400726\n",
      "119 118.75545739783368\n",
      "120 112.39980249305988\n",
      "121 106.40180200007829\n",
      "122 100.74015124462161\n",
      "123 95.3934627252905\n",
      "124 90.3466418267509\n",
      "125 85.57906682897195\n",
      "126 81.07522549459549\n",
      "127 76.81949589745122\n",
      "128 72.7970799241823\n",
      "129 68.99431584390369\n",
      "130 65.39750536509636\n",
      "131 61.99662497973229\n",
      "132 58.780454369601635\n",
      "133 55.73859590371359\n",
      "134 52.85994599017935\n",
      "135 50.135714284454245\n",
      "136 47.558657033742584\n",
      "137 45.11868505531046\n",
      "138 42.80759346179781\n",
      "139 40.61905897792176\n",
      "140 38.54693014243668\n",
      "141 36.585744699086305\n",
      "142 34.726541017493744\n",
      "143 32.96504805632912\n",
      "144 31.296332721633846\n",
      "145 29.714988064586787\n",
      "146 28.216769019587698\n",
      "147 26.796067859083784\n",
      "148 25.448853213849155\n",
      "149 24.171521134503365\n",
      "150 22.960984831822294\n",
      "151 21.813103122340117\n",
      "152 20.723928124277258\n",
      "153 19.690927130617645\n",
      "154 18.710598019134178\n",
      "155 17.78070705416205\n",
      "156 16.898467992097352\n",
      "157 16.06112248947163\n",
      "158 15.266335630935615\n",
      "159 14.511849525195093\n",
      "160 13.795755857092797\n",
      "161 13.115989232437766\n",
      "162 12.470861220309995\n",
      "163 11.857946616071146\n",
      "164 11.275934157587987\n",
      "165 10.72326409364082\n",
      "166 10.198357721149312\n",
      "167 9.699723060388948\n",
      "168 9.226154855732435\n",
      "169 8.776215616027436\n",
      "170 8.34889318860492\n",
      "171 7.942605056241432\n",
      "172 7.556471995878375\n",
      "173 7.189629406104174\n",
      "174 6.840974677373859\n",
      "175 6.509684292451337\n",
      "176 6.194681007773866\n",
      "177 5.8952261189151836\n",
      "178 5.610531583470138\n",
      "179 5.339837233400855\n",
      "180 5.082689406871704\n",
      "181 4.8380665127185285\n",
      "182 4.605351094637518\n",
      "183 4.384026346551841\n",
      "184 4.1735699313547\n",
      "185 3.973481568424408\n",
      "186 3.7831535349873096\n",
      "187 3.602137702160857\n",
      "188 3.4298331932895736\n",
      "189 3.2659077036680646\n",
      "190 3.1100161605884917\n",
      "191 2.961654965406157\n",
      "192 2.8205140532483552\n",
      "193 2.6862303009300565\n",
      "194 2.55844028459493\n",
      "195 2.436828696311158\n",
      "196 2.3210538516115067\n",
      "197 2.210873443347314\n",
      "198 2.1060169787407474\n",
      "199 2.00619712266468\n",
      "200 1.9112065037272912\n",
      "201 1.8207925424326048\n",
      "202 1.7346900091714106\n",
      "203 1.6527047252048437\n",
      "204 1.574668703995965\n",
      "205 1.5003988650869045\n",
      "206 1.4296505127916326\n",
      "207 1.3622837747027696\n",
      "208 1.2981427479652776\n",
      "209 1.237060114201303\n",
      "210 1.1788984932462307\n",
      "211 1.123499988196202\n",
      "212 1.0707290344976377\n",
      "213 1.0204695264347756\n",
      "214 0.9725935541806756\n",
      "215 0.9270102762727186\n",
      "216 0.8835960154342091\n",
      "217 0.8422426703119408\n",
      "218 0.8028259473772893\n",
      "219 0.7652776389089417\n",
      "220 0.7295122945027904\n",
      "221 0.695439923035924\n",
      "222 0.662973510481692\n",
      "223 0.632038550021245\n",
      "224 0.6025688955681269\n",
      "225 0.5744923769613708\n",
      "226 0.547738520544304\n",
      "227 0.5222384207995054\n",
      "228 0.49793719379661255\n",
      "229 0.47477475185655077\n",
      "230 0.45271361552282785\n",
      "231 0.4316853935912719\n",
      "232 0.41164255233019187\n",
      "233 0.3925433281596995\n",
      "234 0.37433153311440803\n",
      "235 0.35697370810825013\n",
      "236 0.34043178614056263\n",
      "237 0.3246638817914338\n",
      "238 0.30962986097315237\n",
      "239 0.2952997505800326\n",
      "240 0.2816380892357803\n",
      "241 0.2686190661505418\n",
      "242 0.2562102273162311\n",
      "243 0.24437419021359175\n",
      "244 0.2330897568870835\n",
      "245 0.2223309020814956\n",
      "246 0.21207846141460707\n",
      "247 0.20229842723248326\n",
      "248 0.19297393559647358\n",
      "249 0.18408578044686286\n",
      "250 0.17560686677509518\n",
      "251 0.16752722453790211\n",
      "252 0.15981752627470372\n",
      "253 0.15246687499985223\n",
      "254 0.14545678695132802\n",
      "255 0.1387700336571852\n",
      "256 0.1323954768893822\n",
      "257 0.12631493181299616\n",
      "258 0.1205160070951246\n",
      "259 0.11498590051039492\n",
      "260 0.10971243569731473\n",
      "261 0.10468308830142176\n",
      "262 0.09988492757961065\n",
      "263 0.0953084571637204\n",
      "264 0.09094282554008715\n",
      "265 0.08677819005153134\n",
      "266 0.08280777677694509\n",
      "267 0.07902083013936981\n",
      "268 0.07540624736717932\n",
      "269 0.07195860283848109\n",
      "270 0.06867113522454082\n",
      "271 0.0655339329581625\n",
      "272 0.06254155561648465\n",
      "273 0.059685721802345285\n",
      "274 0.05696149931649093\n",
      "275 0.05436259052962911\n",
      "276 0.05188348040486855\n",
      "277 0.04951784963195801\n",
      "278 0.04726041213383837\n",
      "279 0.04510696595983289\n",
      "280 0.04305337285442569\n",
      "281 0.04109304855571879\n",
      "282 0.039222368105047964\n",
      "283 0.03743725197783562\n",
      "284 0.03573523973209976\n",
      "285 0.03411025973727376\n",
      "286 0.032559609713259094\n",
      "287 0.03107970536299529\n",
      "288 0.029667543272248076\n",
      "289 0.028320136357636666\n",
      "290 0.027034616262549667\n",
      "291 0.025807631326490582\n",
      "292 0.024636409977293893\n",
      "293 0.023518859201429557\n",
      "294 0.022452071100909146\n",
      "295 0.021434050760422017\n",
      "296 0.020462684123554287\n",
      "297 0.01953550924076793\n",
      "298 0.018650465543437984\n",
      "299 0.01780564644817929\n",
      "300 0.01699970584289559\n",
      "301 0.016230997937724187\n",
      "302 0.015496586370206629\n",
      "303 0.014795518517808375\n",
      "304 0.01412630142537314\n",
      "305 0.013487663532911616\n",
      "306 0.012878087004340892\n",
      "307 0.012296180452415147\n",
      "308 0.011740683327854606\n",
      "309 0.01121041789029798\n",
      "310 0.010704323065549004\n",
      "311 0.010221237887543897\n",
      "312 0.009760156768241553\n",
      "313 0.009319791472906108\n",
      "314 0.00889936507043975\n",
      "315 0.00849810095467154\n",
      "316 0.00811516130955393\n",
      "317 0.007749526939152899\n",
      "318 0.007400338393635598\n",
      "319 0.007066950239708707\n",
      "320 0.006748783854207191\n",
      "321 0.0064449683652712045\n",
      "322 0.006154810441436821\n",
      "323 0.0058779052994913355\n",
      "324 0.005613475324295528\n",
      "325 0.005361031816632358\n",
      "326 0.0051199857647503365\n",
      "327 0.004889793702052091\n",
      "328 0.0046699946554177255\n",
      "329 0.0044601338216026\n",
      "330 0.004259822940650989\n",
      "331 0.004068514022995932\n",
      "332 0.003885821270765602\n",
      "333 0.0037114403500908754\n",
      "334 0.003544881562774933\n",
      "335 0.0033858938665890523\n",
      "336 0.0032340066018130397\n",
      "337 0.0030889615783252543\n",
      "338 0.002950452988707648\n",
      "339 0.002818171634170019\n",
      "340 0.0026918855062802085\n",
      "341 0.0025712603005925636\n",
      "342 0.0024560553353923298\n",
      "343 0.00234604034198238\n",
      "344 0.0022409920876592286\n",
      "345 0.002140673480002886\n",
      "346 0.0020448432922880117\n",
      "347 0.0019533786074298954\n",
      "348 0.0018659964218955014\n",
      "349 0.0017825666452999604\n",
      "350 0.0017028531320554229\n",
      "351 0.0016267182716836505\n",
      "352 0.0015539942457018683\n",
      "353 0.0014845425875572292\n",
      "354 0.0014182266496645915\n",
      "355 0.0013548634930185032\n",
      "356 0.001294353961058633\n",
      "357 0.0012365546438876406\n",
      "358 0.0011813486224428135\n",
      "359 0.0011286249349082356\n",
      "360 0.0010782745984660115\n",
      "361 0.0010301651242345122\n",
      "362 0.0009842025899061792\n",
      "363 0.0009403101784094012\n",
      "364 0.0008983864848926952\n",
      "365 0.0008583571683509618\n",
      "366 0.0008200950621072362\n",
      "367 0.0007835513124566595\n",
      "368 0.0007486413505790757\n",
      "369 0.0007152912841857074\n",
      "370 0.0006834350493407615\n",
      "371 0.0006530006473232432\n",
      "372 0.0006239369076408703\n",
      "373 0.0005961729550853626\n",
      "374 0.0005696402177184118\n",
      "375 0.0005442956898128204\n",
      "376 0.0005200877358547709\n",
      "377 0.0004969571488694424\n",
      "378 0.00047485614278708723\n",
      "379 0.0004537412978342414\n",
      "380 0.0004335743885235213\n",
      "381 0.00041431783170736084\n",
      "382 0.0003959082423581284\n",
      "383 0.0003783190042212421\n",
      "384 0.00036151250778161193\n",
      "385 0.0003454620541207258\n",
      "386 0.000330129348000136\n",
      "387 0.00031547248057343545\n",
      "388 0.00030146848388255565\n",
      "389 0.00028809135467306596\n",
      "390 0.0002753108282246552\n",
      "391 0.0002630972191270831\n",
      "392 0.00025142853786560345\n",
      "393 0.00024027715009774562\n",
      "394 0.0002296237826112543\n",
      "395 0.0002194468055092985\n",
      "396 0.0002097221954350288\n",
      "397 0.000200432796054965\n",
      "398 0.00019155388240753977\n",
      "399 0.0001830705973058019\n",
      "400 0.00017496319847554298\n",
      "401 0.00016721523006782473\n",
      "402 0.00015981097222265945\n",
      "403 0.00015273779224963098\n",
      "404 0.00014597879741706305\n",
      "405 0.00013951850651169214\n",
      "406 0.00013334504318469228\n",
      "407 0.00012744729445259348\n",
      "408 0.00012181016317233518\n",
      "409 0.00011642269188681258\n",
      "410 0.00011127619345545724\n",
      "411 0.00010635746889359584\n",
      "412 0.00010165743441762188\n",
      "413 9.716747337988485e-05\n",
      "414 9.287419226031766e-05\n",
      "415 8.877146107539528e-05\n",
      "416 8.485111163211308e-05\n",
      "417 8.110388207818798e-05\n",
      "418 7.752264820094261e-05\n",
      "419 7.41004921192327e-05\n",
      "420 7.082972190559609e-05\n",
      "421 6.770373572908906e-05\n",
      "422 6.471615435478222e-05\n",
      "423 6.186096958950044e-05\n",
      "424 5.9132127864749485e-05\n",
      "425 5.652451038592757e-05\n",
      "426 5.4031499086427796e-05\n",
      "427 5.164960714665818e-05\n",
      "428 4.9374282972070545e-05\n",
      "429 4.719815734159714e-05\n",
      "430 4.5118203912157596e-05\n",
      "431 4.313023074673282e-05\n",
      "432 4.122989640725796e-05\n",
      "433 3.941365203905413e-05\n",
      "434 3.767780752073371e-05\n",
      "435 3.60187730057083e-05\n",
      "436 3.4432856405590946e-05\n",
      "437 3.291693359226021e-05\n",
      "438 3.146802211661705e-05\n",
      "439 3.0083405799417357e-05\n",
      "440 2.8759768627238944e-05\n",
      "441 2.749430482582524e-05\n",
      "442 2.6285179015768226e-05\n",
      "443 2.512947075132774e-05\n",
      "444 2.4024752260134758e-05\n",
      "445 2.2968427562699554e-05\n",
      "446 2.1958685215457653e-05\n",
      "447 2.0993516994521406e-05\n",
      "448 2.007089390943244e-05\n",
      "449 1.9188757860651567e-05\n",
      "450 1.8345686289506234e-05\n",
      "451 1.7539843312225643e-05\n",
      "452 1.6769343052291145e-05\n",
      "453 1.603283045435248e-05\n",
      "454 1.532890012817514e-05\n",
      "455 1.4655901298029216e-05\n",
      "456 1.4012451106298987e-05\n",
      "457 1.3397365302813263e-05\n",
      "458 1.2809514395723905e-05\n",
      "459 1.2247668180094082e-05\n",
      "460 1.171063201322906e-05\n",
      "461 1.1196864429479375e-05\n",
      "462 1.070578158060058e-05\n",
      "463 1.0236253582784896e-05\n",
      "464 9.787389101022513e-06\n",
      "465 9.358351398778984e-06\n",
      "466 8.948068450154026e-06\n",
      "467 8.555837315000121e-06\n",
      "468 8.18089118830445e-06\n",
      "469 7.822420431423389e-06\n",
      "470 7.479695837899169e-06\n",
      "471 7.152031345498728e-06\n",
      "472 6.838763722497676e-06\n",
      "473 6.539253310928385e-06\n",
      "474 6.252948362804313e-06\n",
      "475 5.9792153529753306e-06\n",
      "476 5.717636990648708e-06\n",
      "477 5.467346387442662e-06\n",
      "478 5.228035275325678e-06\n",
      "479 4.999271794676215e-06\n",
      "480 4.780605655710886e-06\n",
      "481 4.571435765192313e-06\n",
      "482 4.371464110274791e-06\n",
      "483 4.180260024688346e-06\n",
      "484 3.997467582719279e-06\n",
      "485 3.822694574132633e-06\n",
      "486 3.6555542525997314e-06\n",
      "487 3.4957435830779326e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "488 3.3429528421257415e-06\n",
      "489 3.196852382234835e-06\n",
      "490 3.057172409724853e-06\n",
      "491 2.923652152715381e-06\n",
      "492 2.7959777482993336e-06\n",
      "493 2.6738649951215243e-06\n",
      "494 2.5570758983416254e-06\n",
      "495 2.445401363449915e-06\n",
      "496 2.33862960731558e-06\n",
      "497 2.2365362572224243e-06\n",
      "498 2.1388982867884046e-06\n",
      "499 2.0455437177485622e-06\n"
     ]
    }
   ],
   "source": [
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# N: batch_size, H: hidden dims\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = np.random.randn(N, D_in) # (64, 1000)\n",
    "y = np.random.randn(N, D_out) # (64, 10)\n",
    "\n",
    "w1 = np.random.randn(D_in, H) # (1000, 100)\n",
    "w2 = np.random.randn(H, D_out) # (100, 10)\n",
    "\n",
    "lr = 1e-6 \n",
    "for t in range(500):\n",
    "    # Forward pass: compute predicted y \n",
    "    h = x.dot(w1)\n",
    "    h_relu = np.maximum(h, 0)\n",
    "    y_pred = h_relu.dot(w2)\n",
    "    \n",
    "    # Compute and print loss \n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    print(t, loss)\n",
    "    \n",
    "    # Backprop to compute gradients of w1 and w2 with respect to loss \n",
    "    grad_y_pred = 2.0 * (y_pred - y) # (64, 10)\n",
    "    grad_w2 = h_relu.T.dot(grad_y_pred) # (100, 10)\n",
    "    grad_h_relu = grad_y_pred.dot(w2.T) # (64, 100)\n",
    "    grad_h = grad_h_relu.copy()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.T.dot(grad_h) # (1000, 100)\n",
    "    \n",
    "    # Update weigths \n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 10363.41796875\n",
      "150 17.94891929626465\n",
      "250 0.07549849152565002\n",
      "350 0.0006497364956885576\n",
      "450 6.203429074957967e-05\n"
     ]
    }
   ],
   "source": [
    "# PyTorch\n",
    "# numpy is not utilize GPUs to accelerate its numerical computations \n",
    "# pytorch Tensors can keep track of a computational graph and gradients, utilize GPU\n",
    "\n",
    "import torch \n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype)\n",
    "\n",
    "lr = 1e-6\n",
    "\n",
    "for t in range(500):\n",
    "    h = x.mm(w1)\n",
    "    h_relu = h.clamp(min=0)\n",
    "    y_pred = h_relu.mm(w2)\n",
    "    \n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 50:\n",
    "        print(t, loss)\n",
    "    \n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_w2 = h_relu.t().mm(grad_y_pred)\n",
    "    grad_h_relu = grad_y_pred.mm(w2.t())\n",
    "    grad_h = grad_h_relu.clone()\n",
    "    grad_h[h < 0] = 0\n",
    "    grad_w1 = x.t().mm(grad_h)\n",
    "    \n",
    "    w1 -= lr * grad_w1\n",
    "    w2 -= lr * grad_w2 \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Autograd\n",
    "- When using autograd, the forward pass of your network will define a computational graph; nodes in the graph will be Tensors, and edges will be functions that produce output Tensors from input Tensors. Backpropagating through this graph then allows you to easily compute gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 650.3226318359375\n",
      "199 3.5059964656829834\n",
      "299 0.02962470054626465\n",
      "399 0.0005231481627561152\n",
      "499 6.611215940210968e-05\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64 ,1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "lr = 1e-6\n",
    "for t in range(500):\n",
    "    y_pred = x.mm(w1).clamp(min=0).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "    \n",
    "    loss.backward() # backpropagating\n",
    "    \n",
    "    # we don't track weight.data. So, wrap in torch.no_grad()\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        \n",
    "        # manually zero the gradients after updating weights\n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Defining new autograd functions\n",
    "- In PyTorch we can easily define our own autograd operator by defining a subclass of torch.autograd.Function and implementing the forward and backward functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class MyReLU(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forwad pass we receive a Tensor containing the input and return \n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache arbitrary \n",
    "        onjects for use in the backward pass using the ctx.save_for_backward method.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return input.clamp(min=0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss \n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        grad_input = grad_output.clone()\n",
    "        grad_input[input < 0] = 0\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499 841.0775146484375\n",
      "499 8.00600814819336\n",
      "499 0.1287042498588562\n",
      "499 0.0028903367929160595\n",
      "499 0.00021001743152737617\n"
     ]
    }
   ],
   "source": [
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "N, D_in, H, D_out = 64 ,1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in, device=device, dtype=dtype)\n",
    "y = torch.randn(N, D_out, device=device, dtype=dtype)\n",
    "\n",
    "w1 = torch.randn(D_in, H, device=device, dtype=dtype, requires_grad=True)\n",
    "w2 = torch.randn(H, D_out, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "lr = 1e-6\n",
    "for epoch in range(500):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n",
    "    relu = MyReLU.apply\n",
    "    \n",
    "    y_pred = relu(x.mm(w1)).mm(w2)\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if epoch % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "        \n",
    "    # Use autograd to compute the backward pass.        \n",
    "    loss.backward()\n",
    "    \n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        w1 -= lr * w1.grad\n",
    "        w2 -= lr * w2.grad\n",
    "        \n",
    "        w1.grad.zero_()\n",
    "        w2.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: nn module\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.4625134468078613\n",
      "199 0.04482162371277809\n",
      "299 0.0016880746698006988\n",
      "399 8.578517008572817e-05\n",
      "499 4.802728653885424e-06\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "\n",
    "lr = 1e-4\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x)\n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 100 == 99:\n",
    "        print(epoch, loss.item())\n",
    "        \n",
    "    # Zero the gradients before running the backward pass.\n",
    "    model.zero_grad()\n",
    "    \n",
    "    # Backward pass: compute gradient of the loss with respect to all the learnable\n",
    "    # parameters of the model. Internally, the parameters of each Module are stored\n",
    "    # in Tensors with requires_grad=True, so this call will compute gradients for\n",
    "    # all learnable parameters in the model\n",
    "    loss.backward()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for param in model.parameters():\n",
    "            param -= lr* param.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: optim\n",
    "- Up to this point we have updated the weights of our models by manually mutating the Tensors holding learnable parameters (with torch.no_grad() or .data to avoid tracking history in autograd). This is not a huge burden for simple optimization algorithms like stochastic gradient descent, but in practice we often train neural networks using more sophisticated optimizers like AdaGrad, RMSProp, Adam, etc.\n",
    "- The optim package in PyTorch abstracts the idea of an optimization algorithm and provides implementations of commonly used optimization algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 64.52503967285156\n",
      "199 2.1026833057403564\n",
      "299 0.03910357132554054\n",
      "399 0.00018847813771571964\n",
      "499 1.8031380477623316e-07\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(D_in, H),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(H, D_out),\n",
    ")\n",
    "loss_fn = torch.nn.MSELoss(reduction=\"sum\")\n",
    "\n",
    "# Use the optim package to define an Optimizer that will update the weights of the model for us\n",
    "lr = 1e-4\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = loss_fn(y_pred, y)\n",
    "    if epoch % 100 == 99:\n",
    "        print(epoch, loss.item())\n",
    "        \n",
    "    # Before the backward pass, use the optimizer object to zero all of the gradients\n",
    "    # for the variables it will update.\n",
    "    # this is because by default, gradients are accumulated in bufffers(i.e, not overwritten)\n",
    "    # whenever .backward() is called. \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    # Calling the step function, an Optimizer makes an update to its parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Custom nn Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as \n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return \n",
    "        a Tensor of output data. we can use Modules defined in the constructor as \n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 2.5754308700561523\n",
      "199 0.044577814638614655\n",
      "299 0.0019828921649605036\n",
      "399 0.0001189532849821262\n",
      "499 7.833315976313315e-06\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = TwoLayerNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if epoch % 100 == 99:\n",
    "        print(epoch, loss.item()) # Returns the value of this tensor as a standard Python number. \n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch: Control Flow + Weight Sharing\n",
    "- As an example of dynamic graphs and weight sharing, we implement a very strange model: a fully-connected ReLU network that on each forward pass chooses a random number between 1 and 4 and uses that many hidden layers, reusing the same weights multiple times to compute the innermost hidden layers.\n",
    "\n",
    "- For this model we can use normal Python flow control to implement the loop, and we can implement weight sharing among the innermost layers by simply reusing the same Module multiple times when defining the forward pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import torch\n",
    "\n",
    "class DynamicNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        super(DynamicNet, self).__init__()\n",
    "        self.input_linear = torch.nn.Linear(D_in, H)\n",
    "        self.middle_linear = torch.nn.Linear(H, H)\n",
    "        self.output_linear = torch.nn.Linear(H, D_out)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        For the forward pass of the model, we randomly choose either 0, 1, 2, or 3\n",
    "        and reuse the middle_linear Module that many times to compute hidden layer\n",
    "        representations.\n",
    "        \n",
    "        Since each forward pass builds a dynamic computation graph, we can use normal\n",
    "        Python control-flow operators like loops or conditional statements when\n",
    "        defining the forward pass of the model.\n",
    "\n",
    "        Here we also see that it is perfectly safe to reuse the same Module many\n",
    "        times when defining a computational graph. This is a big improvement from Lua\n",
    "        Torch, where each Module could be used only once.\n",
    "        \"\"\"\n",
    "        h_relu = self.input_linear(x).clamp(min=0)\n",
    "        for _ in range(random.randint(0, 3)):\n",
    "            h_relu = self.middle_linear(h_relu).clamp(min=0)\n",
    "        y_pred = self.output_linear(h_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 10.051538467407227\n",
      "199 3.7825474739074707\n",
      "299 0.7820899486541748\n",
      "399 3.851109266281128\n",
      "499 17.0799617767334\n"
     ]
    }
   ],
   "source": [
    "N, D_in, H, D_out = 64, 1000, 100, 10\n",
    "\n",
    "x = torch.randn(N, D_in)\n",
    "y = torch.randn(N, D_out)\n",
    "\n",
    "model = DynamicNet(D_in, H, D_out)\n",
    "\n",
    "criterion = torch.nn.MSELoss(reduction=\"sum\")\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4, momentum=0.9)\n",
    "\n",
    "for epoch in range(500):\n",
    "    y_pred = model(x)\n",
    "    \n",
    "    loss = criterion(y_pred, y)\n",
    "    if epoch % 100 == 99:\n",
    "        print(epoch, loss.item())\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
